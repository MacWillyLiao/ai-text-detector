{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IsdMMs9CUL4"
      },
      "source": [
        "- run in Google Colab and use T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hDscU5N4rg0s",
        "outputId": "3bdee822-17cc-48fe-92c6-de1cbd8e5751"
      },
      "outputs": [],
      "source": [
        "!pip install flask\n",
        "!pip install flask_cors\n",
        "!pip install flask_ngrok\n",
        "!pip install pyngrok\n",
        "!pip install --upgrade simpletransformers\n",
        "!pip install torchvision\n",
        "!pip install -U ckip-transformers\n",
        "!pip install captum==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ihYz-L-7rcLD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS  # 載入 CORS\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from google.colab import drive\n",
        "from captum.attr import IntegratedGradients\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertConfig\n",
        "import gc\n",
        "import math\n",
        "import threading  # Import the threading module\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igEvPDn7MgEq"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # 為了解決 Ram 空間不夠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SJ_SyYFq5io"
      },
      "outputs": [],
      "source": [
        "ngrok.set_auth_token(\"Your Ngork Token\")  # 替換為你的 ngrok token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8AjO8d3m6Ll",
        "outputId": "731bdbb0-436f-4b1d-bce1-2b2dee1d05b0"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')  # 連接至 Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jHiPjkv5P4BJ",
        "outputId": "3b1f4089-e854-4194-9cb0-12998b5ecdc4"
      },
      "outputs": [],
      "source": [
        "# 預設模型與解釋器\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/complete_model1.pt\"\n",
        "model = torch.load(model_path, weights_only=False, map_location=device)  # 載入訓練好的模型\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"ckiplab/bert-base-chinese\", num_labels=2)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device).eval()\n",
        "\n",
        "model.config = BertConfig.from_pretrained('ckiplab/bert-base-chinese')\n",
        "tokenizer = BertTokenizer.from_pretrained('ckiplab/bert-base-chinese')\n",
        "ig = IntegratedGradients(forward_func=lambda x, mask: model(inputs_embeds=x, attention_mask=mask).logits.softmax(dim=1)[:, 1])\n",
        "\n",
        "# 計算每個 token 的重要性（貢獻度）分數\n",
        "def get_token_importances(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    embeddings = model.bert.embeddings(input_ids)\n",
        "\n",
        "    baseline = torch.zeros_like(embeddings)\n",
        "    attributions, _ = ig.attribute(inputs=embeddings,\n",
        "                                   baselines=baseline,\n",
        "                                   additional_forward_args=(attention_mask,),\n",
        "                                   return_convergence_delta=True)\n",
        "\n",
        "    token_attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "    token_list = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    token_scores = token_attributions.detach().cpu().tolist()\n",
        "\n",
        "    del inputs, input_ids, attention_mask, embeddings, attributions, token_attributions\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return list(zip(token_list, token_scores))\n",
        "\n",
        "# 預測函數\n",
        "def predict(input_sentence):\n",
        "    inputs = tokenizer(input_sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1)  # 選擇每個樣本得分最高的索引\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    probability = probabilities[0, prediction].item()\n",
        "\n",
        "    tokens_with_scores = get_token_importances(input_sentence)  # 計算貢獻值\n",
        "\n",
        "    # 對分數做操作\n",
        "    tokens_with_scores = [\n",
        "        (tok, round(float(score) * 100, 4))  # 乘以100；四捨五入到小數點後4位\n",
        "        for tok, score in tokens_with_scores\n",
        "        if math.isfinite(score)  # 排除 NaN 和 Inf\n",
        "    ]\n",
        "\n",
        "    del inputs, outputs, logits, probabilities\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    prob_rounded = math.floor(probability * 100) / 100  # 無條件捨去到小數點第二位\n",
        "    return prediction.item(), prob_rounded, tokens_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xbffDhV6CUMG",
        "outputId": "638bc479-a560-4a26-80eb-e021937c9c87"
      },
      "outputs": [],
      "source": [
        "# 建立 Flask\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# 啟動 ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Flask app is publicly accessible at:\", public_url)\n",
        "\n",
        "@app.route('/process', methods=['POST'])\n",
        "def process_text():\n",
        "    data = request.get_json()\n",
        "    input_sentence = data.get('text')\n",
        "    prediction, probability, tokens_with_scores = predict(input_sentence)\n",
        "    tokens_list = [{'token': tok, 'score': score} for tok, score in tokens_with_scores if tok not in ('[CLS]', '[SEP]')]\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    return jsonify({'prediction': prediction, 'probability': probability, 'tokens': tokens_list})\n",
        "\n",
        "# ——————— 解決 RAM 空間不夠 ———————\n",
        "@app.route('/restart', methods=['POST'])\n",
        "def restart():\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "# GPU 記憶體監控清理\n",
        "def clean_memory_periodically():\n",
        "    while True:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        time.sleep(30)\n",
        "\n",
        "threading.Thread(target=clean_memory_periodically, daemon=True).start()\n",
        "# ———————————————————————————————\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
